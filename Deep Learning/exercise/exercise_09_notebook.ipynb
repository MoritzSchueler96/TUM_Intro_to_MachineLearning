{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "# Convert a categorical vector y (shape [N]) into a one-hot encoded matrix (shape [N, K])\n",
    "Y = label_binarize(y, np.unique(y)).astype(np.float64)\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, K = Y.shape  # N - num_samples, K - num_classes\n",
    "D = X.shape[1] # num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from the tutorial:\n",
    "1. No for loops! Use matrix multiplication and broadcasting whenever possible.\n",
    "2. Think about numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn_utils  # module containing helper functions for checking the correctness of your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Affine layer\n",
    "Implement `forward` and `backward` functions for `Affine` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def forward(self, inputs, weight, bias):\n",
    "        \"\"\"Forward pass of an affine (fully connected) layer.\n",
    "\n",
    "        Args:\n",
    "            inputs: input matrix, shape (N, D)\n",
    "            weight: weight matrix, shape (D, H)\n",
    "            bias: bias vector, shape (H)\n",
    "\n",
    "        Returns\n",
    "            out: output matrix, shape (N, H)\n",
    "        \"\"\"\n",
    "        self.cache = (inputs, weight, bias)\n",
    "        #############################################################\n",
    "        # TODO\n",
    "        N = np.shape(inputs)[0]\n",
    "        [D, _] = np.shape(weight)\n",
    "\n",
    "        x_trans = np.reshape(inputs, (N, D))\n",
    "        out = x_trans@weight + bias\n",
    "        #############################################################\n",
    "        assert out.shape[0] == inputs.shape[0]\n",
    "        assert out.shape[1] == weight.shape[1] == bias.shape[0]\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"Backward pass of an affine (fully connected) layer.\n",
    "\n",
    "        Args:\n",
    "            d_out: incoming derivaties, shape (N, H)\n",
    "\n",
    "        Returns:\n",
    "            d_inputs: gradient w.r.t. the inputs, shape (N, D)\n",
    "            d_weight: gradient w.r.t. the weight, shape (D, H)\n",
    "            d_bias: gradient w.r.t. the bias, shape (H)\n",
    "        \"\"\"\n",
    "        inputs, weight, bias = self.cache\n",
    "        #############################################################\n",
    "        # TODO\n",
    "        N = np.shape(inputs)[0]\n",
    "\n",
    "        d_inputs = d_out @ weight.T\n",
    "        d_weight = inputs.T @ d_out\n",
    "        d_bias = d_out.T @ np.ones(N)\n",
    "        \n",
    "        #############################################################\n",
    "        assert np.all(d_inputs.shape == inputs.shape)\n",
    "        assert np.all(d_weight.shape == weight.shape)\n",
    "        assert np.all(d_bias.shape == bias.shape)\n",
    "        return d_inputs, d_weight, d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed succesfully!\n"
     ]
    }
   ],
   "source": [
    "affine = Affine()\n",
    "nn_utils.check_affine(affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: ReLU layer\n",
    "Implement `forward` and `backward` functions for `ReLU` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of a ReLU layer.\n",
    "\n",
    "        Args:\n",
    "            inputs: input matrix, arbitrary shape\n",
    "\n",
    "        Returns:\n",
    "            out: output matrix, has same shape as inputs\n",
    "        \"\"\"\n",
    "        self.cache = inputs\n",
    "        #############################################################\n",
    "        # TODO\n",
    "        out = np.maximum(0, inputs)\n",
    "        #############################################################\n",
    "        assert np.all(out.shape == inputs.shape)\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        \"\"\"Backward pass of an ReLU layer.\n",
    "\n",
    "        Args:\n",
    "            d_out: incoming derivatives, same shape as inputs in forward\n",
    "\n",
    "        Returns:\n",
    "            d_inputs: gradient w.r.t. the inputs, same shape as d_out\n",
    "        \"\"\"\n",
    "        inputs = self.cache\n",
    "        #############################################################\n",
    "        # TODO\n",
    "        d_inputs = np.zeros_like(inputs)\n",
    "        d_inputs[inputs > 0] = 1\n",
    "        d_inputs = d_out * d_inputs\n",
    "        #############################################################\n",
    "        assert np.all(d_inputs.shape == inputs.shape)\n",
    "        return d_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed succesfully!\n"
     ]
    }
   ],
   "source": [
    "relu = ReLU()\n",
    "nn_utils.check_relu(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: CategoricalCrossEntropy layer\n",
    "Implement `forward` and `backward` for `CategoricalCrossEntropy` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropy:\n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"Compute categorical cross-entropy loss.\n",
    "\n",
    "        Args:\n",
    "            logits: class logits, shape (N, K)\n",
    "            labels: target labels in one-hot format, shape (N, K)\n",
    "\n",
    "        Returns:\n",
    "            loss: loss value, float (a single number)\n",
    "        \"\"\"\n",
    "        #############################################################\n",
    "        # TODO        \n",
    "\n",
    "        N = np.shape(logits)[0]\n",
    "        probs = softmax(logits, axis=1)\n",
    "        loss = - 1/N * np.sum(labels * np.log(probs))\n",
    "        \n",
    "        #############################################################\n",
    "        # probs is the (N, K) matrix of class probabilities\n",
    "        self.cache = (probs, labels)\n",
    "        assert isinstance(loss, float)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, d_out=1.0):\n",
    "        \"\"\"Backward pass of the Cross Entropy loss.\n",
    "\n",
    "        Args:\n",
    "            d_out: Incoming derivatives. We set this value to 1.0 by default,\n",
    "                since this is the terminal node of our computational graph\n",
    "                (i.e. we usually want to compute gradients of loss w.r.t.\n",
    "                other model parameters).\n",
    "\n",
    "        Returns:\n",
    "            d_logits: gradient w.r.t. the logits, shape (N, K)\n",
    "            d_labels: gradient w.r.t. the labels\n",
    "                we don't need d_labels for our models, so we don't\n",
    "                compute it and set it to None. It's only included in the\n",
    "                function definition for consistency with other layers.\n",
    "        \"\"\"\n",
    "        probs, labels = self.cache\n",
    "        #############################################################\n",
    "        # TODO\n",
    "        N = np.shape(labels)[0]\n",
    "        \n",
    "        y_he = labels\n",
    "        y = np.argmax(y_he, axis=1)\n",
    "        d_logits = probs.copy()\n",
    "        d_logits[np.arange(N), y] -= 1\n",
    "        d_logits = d_logits * d_out / N\n",
    "        \n",
    "        #d_logits = (probs - labels) * d_out / N\n",
    "        \n",
    "        #############################################################\n",
    "        d_labels = None\n",
    "        assert np.all(d_logits.shape == probs.shape == labels.shape)\n",
    "        return d_logits, d_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All checks passed succesfully!\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = CategoricalCrossEntropy()\n",
    "nn_utils.check_cross_entropy(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression (with backpropagation) --- nothing to do in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=1e-2):\n",
    "        \"\"\"Logistic regression model.\n",
    "        Gradients are computed with backpropagation.\n",
    "\n",
    "\n",
    "        The model consists of the following sequence of opeartions:\n",
    "        \n",
    "        input -> affine -> softmax\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize the model parameters\n",
    "        self.params = {\n",
    "            'W': np.zeros([num_features, num_classes]),\n",
    "            'b': np.zeros([num_classes])\n",
    "        }\n",
    "        \n",
    "        # Define layers\n",
    "        self.affine = Affine()\n",
    "        self.cross_entropy = CategoricalCrossEntropy()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate predictions for one minibatch.\n",
    "        \n",
    "        Args:\n",
    "            X: data matrix, shape (N, D)\n",
    "            \n",
    "        Returns:\n",
    "            Y_pred: predicted class probabilities, shape (N, D)\n",
    "            Y_pred[n, k] = probability that sample n belongs to class k\n",
    "        \"\"\"\n",
    "        logits = self.affine.forward(X,self.params['W'], self.params['b'])\n",
    "        Y_pred = softmax(logits, axis=1)\n",
    "        return Y_pred\n",
    "    \n",
    "    def step(self, X, Y):\n",
    "        \"\"\"Perform one step of gradient descent on the minibatch of data.\n",
    "        \n",
    "        1. Compute the cross-entropy loss for given (X, Y).\n",
    "        2. Compute the gradients of the loss w.r.t. model parameters.\n",
    "        3. Update the model parameters using the gradients.\n",
    "        \n",
    "        Args:\n",
    "            X: data matrix, shape (N, D)\n",
    "            Y: target labels in one-hot format, shape (N, K)\n",
    "            \n",
    "        Returns:\n",
    "            loss: loss for (X, Y), float, (a single number)\n",
    "        \"\"\"\n",
    "        # Forward pass - compute the loss on training data\n",
    "        logits = self.affine.forward(X, self.params['W'], self.params['b'])\n",
    "        loss = self.cross_entropy.forward(logits, Y)\n",
    "        \n",
    "        # Backward pass - compute the gradients of loss w.r.t. all the model parameters\n",
    "        grads = {}\n",
    "        d_logits, _ = self.cross_entropy.backward()\n",
    "        _, grads['W'], grads['b'] = self.affine.backward(d_logits)\n",
    "        \n",
    "        # Apply the gradients\n",
    "        for p in self.params:\n",
    "            self.params[p] = self.params[p] - self.learning_rate * grads[p]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimization parameters\n",
    "learning_rate = 1e-2\n",
    "max_epochs = 501\n",
    "report_frequency = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(num_features=D, num_classes=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss = 2.3026\n",
      "Epoch   50, loss = 0.2275\n",
      "Epoch  100, loss = 0.1599\n",
      "Epoch  150, loss = 0.1306\n",
      "Epoch  200, loss = 0.1130\n",
      "Epoch  250, loss = 0.1009\n",
      "Epoch  300, loss = 0.0918\n",
      "Epoch  350, loss = 0.0846\n",
      "Epoch  400, loss = 0.0788\n",
      "Epoch  450, loss = 0.0738\n",
      "Epoch  500, loss = 0.0696\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    loss = log_reg.step(X_train, Y_train)\n",
    "    if epoch % report_frequency == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = log_reg.predict(X_test).argmax(1)\n",
    "y_test_true = Y_test.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy = 0.953\n"
     ]
    }
   ],
   "source": [
    "print(f'test set accuracy = {accuracy_score(y_test_true, y_test_pred):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward neural network (with backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(shape):\n",
    "    \"\"\"Initialize a weight matrix according to Xavier initialization.\n",
    "    \n",
    "    See pytorch.org/docs/stable/nn.init#torch.nn.init.xavier_uniform_ for details.\n",
    "    \"\"\"\n",
    "    a = np.sqrt(6.0 / float(np.sum(shape)))\n",
    "    return np.random.uniform(low=-a, high=a, size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement a two-layer `FeedForwardNeuralNet` model\n",
    "You can use the `LogisticRegression` class for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=1e-2):\n",
    "        \"\"\"A two-layer feedforward neural network with ReLU activations.\n",
    "        \n",
    "        (input_layer -> hidden_layer -> output_layer)\n",
    "        \n",
    "        \n",
    "        The model consists of the following sequence of opeartions:\n",
    "        \n",
    "        input -> affine -> relu -> affine -> softmax\n",
    "        \n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Initialize the model parameters\n",
    "        self.params = {\n",
    "            'W1': xavier_init([input_size, hidden_size]),\n",
    "            'b1': np.zeros([hidden_size]),\n",
    "            'W2': xavier_init([hidden_size, output_size]),\n",
    "            'b2': np.zeros([output_size]),\n",
    "        }\n",
    "        \n",
    "        # Define layers\n",
    "        ############################################################\n",
    "        # TODO\n",
    "        self.affine = Affine()\n",
    "        self.relu = ReLU()\n",
    "        self.affine2 = Affine()\n",
    "        self.cross_entropy = CategoricalCrossEntropy()\n",
    "        \n",
    "        ############################################################\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate predictions for one minibatch.\n",
    "        \n",
    "        Args:\n",
    "            X: data matrix, shape (N, D)\n",
    "            \n",
    "        Returns:\n",
    "            Y_pred: predicted class probabilities, shape (N, D)\n",
    "            Y_pred[n, k] = probability that sample n belongs to class k\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        # TODO\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "        logits1 = self.affine.forward(X, W1, b1)\n",
    "        logits2 = self.relu.forward(logits1)\n",
    "        logits3 = self.affine.forward(logits2, W2, b2)\n",
    "        Y_pred = softmax(logits3, axis=1)\n",
    "        \n",
    "        ############################################################\n",
    "        return Y_pred\n",
    "    \n",
    "    def step(self, X, Y):\n",
    "        \"\"\"Perform one step of gradient descent on the minibatch of data.\n",
    "        \n",
    "        1. Compute the cross-entropy loss for given (X, Y).\n",
    "        2. Compute the gradients of the loss w.r.t. model parameters.\n",
    "        3. Update the model parameters using the gradients.\n",
    "        \n",
    "        Args:\n",
    "            X: data matrix, shape (N, D)\n",
    "            Y: target labels in one-hot format, shape (N, K)\n",
    "            \n",
    "        Returns:\n",
    "            loss: loss for (X, Y), float, (a single number)\n",
    "        \"\"\"\n",
    "        ############################################################\n",
    "        # TODO\n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "        # calculate forward pass\n",
    "        logits1 = self.affine.forward(X, W1, b1)\n",
    "        logits2 = self.relu.forward(logits1)\n",
    "        logits3 = self.affine2.forward(logits2, W2, b2)\n",
    "        loss = self.cross_entropy.forward(logits3, Y)\n",
    "        \n",
    "        # calculate gradients\n",
    "        grads = {}\n",
    "        d_loss, _ = self.cross_entropy.backward()\n",
    "        dlogits3, grads['W2'], grads['b2'] = self.affine2.backward(d_loss)\n",
    "        dlogits2 = self.relu.backward(dlogits3)\n",
    "        dlogits1, grads['W1'], grads['b1'] = self.affine.backward(dlogits2)\n",
    "        \n",
    "        # Apply the gradients\n",
    "        for p in self.params:\n",
    "            self.params[p] = self.params[p] - self.learning_rate * grads[p]\n",
    "        \n",
    "        ############################################################\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 500 #32  # size of the hidden layer\n",
    "\n",
    "# Specify optimization parameters\n",
    "learning_rate = 1e-1\n",
    "max_epochs = 8501\n",
    "report_frequency = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedforwardNeuralNet(input_size=D, hidden_size=H, output_size=K, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0, loss = 6.7194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n",
      "/home/moritz/anaconda3/envs/MachineLearning/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   50, loss = 0.0704\n",
      "Epoch  100, loss = 0.0326\n",
      "Epoch  150, loss = 0.0197\n",
      "Epoch  200, loss = 0.0136\n",
      "Epoch  250, loss = 0.0102\n",
      "Epoch  300, loss = 0.0081\n",
      "Epoch  350, loss = 0.0066\n",
      "Epoch  400, loss = 0.0056\n",
      "Epoch  450, loss = 0.0048\n",
      "Epoch  500, loss = 0.0042\n",
      "Epoch  550, loss = 0.0038\n",
      "Epoch  600, loss = 0.0034\n",
      "Epoch  650, loss = 0.0031\n",
      "Epoch  700, loss = 0.0028\n",
      "Epoch  750, loss = 0.0026\n",
      "Epoch  800, loss = 0.0024\n",
      "Epoch  850, loss = 0.0022\n",
      "Epoch  900, loss = 0.0020\n",
      "Epoch  950, loss = 0.0019\n",
      "Epoch 1000, loss = 0.0018\n",
      "Epoch 1050, loss = 0.0017\n",
      "Epoch 1100, loss = 0.0016\n",
      "Epoch 1150, loss = 0.0015\n",
      "Epoch 1200, loss = 0.0014\n",
      "Epoch 1250, loss = 0.0014\n",
      "Epoch 1300, loss = 0.0013\n",
      "Epoch 1350, loss = 0.0013\n",
      "Epoch 1400, loss = 0.0012\n",
      "Epoch 1450, loss = 0.0012\n",
      "Epoch 1500, loss = 0.0011\n",
      "Epoch 1550, loss = 0.0011\n",
      "Epoch 1600, loss = 0.0010\n",
      "Epoch 1650, loss = 0.0010\n",
      "Epoch 1700, loss = 0.0009\n",
      "Epoch 1750, loss = 0.0009\n",
      "Epoch 1800, loss = 0.0009\n",
      "Epoch 1850, loss = 0.0009\n",
      "Epoch 1900, loss = 0.0008\n",
      "Epoch 1950, loss = 0.0008\n",
      "Epoch 2000, loss = 0.0008\n",
      "Epoch 2050, loss = 0.0008\n",
      "Epoch 2100, loss = 0.0007\n",
      "Epoch 2150, loss = 0.0007\n",
      "Epoch 2200, loss = 0.0007\n",
      "Epoch 2250, loss = 0.0007\n",
      "Epoch 2300, loss = 0.0007\n",
      "Epoch 2350, loss = 0.0006\n",
      "Epoch 2400, loss = 0.0006\n",
      "Epoch 2450, loss = 0.0006\n",
      "Epoch 2500, loss = 0.0006\n",
      "Epoch 2550, loss = 0.0006\n",
      "Epoch 2600, loss = 0.0006\n",
      "Epoch 2650, loss = 0.0006\n",
      "Epoch 2700, loss = 0.0005\n",
      "Epoch 2750, loss = 0.0005\n",
      "Epoch 2800, loss = 0.0005\n",
      "Epoch 2850, loss = 0.0005\n",
      "Epoch 2900, loss = 0.0005\n",
      "Epoch 2950, loss = 0.0005\n",
      "Epoch 3000, loss = 0.0005\n",
      "Epoch 3050, loss = 0.0005\n",
      "Epoch 3100, loss = 0.0005\n",
      "Epoch 3150, loss = 0.0005\n",
      "Epoch 3200, loss = 0.0004\n",
      "Epoch 3250, loss = 0.0004\n",
      "Epoch 3300, loss = 0.0004\n",
      "Epoch 3350, loss = 0.0004\n",
      "Epoch 3400, loss = 0.0004\n",
      "Epoch 3450, loss = 0.0004\n",
      "Epoch 3500, loss = 0.0004\n",
      "Epoch 3550, loss = 0.0004\n",
      "Epoch 3600, loss = 0.0004\n",
      "Epoch 3650, loss = 0.0004\n",
      "Epoch 3700, loss = 0.0004\n",
      "Epoch 3750, loss = 0.0004\n",
      "Epoch 3800, loss = 0.0004\n",
      "Epoch 3850, loss = 0.0004\n",
      "Epoch 3900, loss = 0.0004\n",
      "Epoch 3950, loss = 0.0004\n",
      "Epoch 4000, loss = 0.0003\n",
      "Epoch 4050, loss = 0.0003\n",
      "Epoch 4100, loss = 0.0003\n",
      "Epoch 4150, loss = 0.0003\n",
      "Epoch 4200, loss = 0.0003\n",
      "Epoch 4250, loss = 0.0003\n",
      "Epoch 4300, loss = 0.0003\n",
      "Epoch 4350, loss = 0.0003\n",
      "Epoch 4400, loss = 0.0003\n",
      "Epoch 4450, loss = 0.0003\n",
      "Epoch 4500, loss = 0.0003\n",
      "Epoch 4550, loss = 0.0003\n",
      "Epoch 4600, loss = 0.0003\n",
      "Epoch 4650, loss = 0.0003\n",
      "Epoch 4700, loss = 0.0003\n",
      "Epoch 4750, loss = 0.0003\n",
      "Epoch 4800, loss = 0.0003\n",
      "Epoch 4850, loss = 0.0003\n",
      "Epoch 4900, loss = 0.0003\n",
      "Epoch 4950, loss = 0.0003\n",
      "Epoch 5000, loss = 0.0003\n",
      "Epoch 5050, loss = 0.0003\n",
      "Epoch 5100, loss = 0.0003\n",
      "Epoch 5150, loss = 0.0003\n",
      "Epoch 5200, loss = 0.0003\n",
      "Epoch 5250, loss = 0.0003\n",
      "Epoch 5300, loss = 0.0003\n",
      "Epoch 5350, loss = 0.0002\n",
      "Epoch 5400, loss = 0.0002\n",
      "Epoch 5450, loss = 0.0002\n",
      "Epoch 5500, loss = 0.0002\n",
      "Epoch 5550, loss = 0.0002\n",
      "Epoch 5600, loss = 0.0002\n",
      "Epoch 5650, loss = 0.0002\n",
      "Epoch 5700, loss = 0.0002\n",
      "Epoch 5750, loss = 0.0002\n",
      "Epoch 5800, loss = 0.0002\n",
      "Epoch 5850, loss = 0.0002\n",
      "Epoch 5900, loss = 0.0002\n",
      "Epoch 5950, loss = 0.0002\n",
      "Epoch 6000, loss = 0.0002\n",
      "Epoch 6050, loss = 0.0002\n",
      "Epoch 6100, loss = 0.0002\n",
      "Epoch 6150, loss = 0.0002\n",
      "Epoch 6200, loss = 0.0002\n",
      "Epoch 6250, loss = 0.0002\n",
      "Epoch 6300, loss = 0.0002\n",
      "Epoch 6350, loss = 0.0002\n",
      "Epoch 6400, loss = 0.0002\n",
      "Epoch 6450, loss = 0.0002\n",
      "Epoch 6500, loss = 0.0002\n",
      "Epoch 6550, loss = 0.0002\n",
      "Epoch 6600, loss = 0.0002\n",
      "Epoch 6650, loss = 0.0002\n",
      "Epoch 6700, loss = 0.0002\n",
      "Epoch 6750, loss = 0.0002\n",
      "Epoch 6800, loss = 0.0002\n",
      "Epoch 6850, loss = 0.0002\n",
      "Epoch 6900, loss = 0.0002\n",
      "Epoch 6950, loss = 0.0002\n",
      "Epoch 7000, loss = 0.0002\n",
      "Epoch 7050, loss = 0.0002\n",
      "Epoch 7100, loss = 0.0002\n",
      "Epoch 7150, loss = 0.0002\n",
      "Epoch 7200, loss = 0.0002\n",
      "Epoch 7250, loss = 0.0002\n",
      "Epoch 7300, loss = 0.0002\n",
      "Epoch 7350, loss = 0.0002\n",
      "Epoch 7400, loss = 0.0002\n",
      "Epoch 7450, loss = 0.0002\n",
      "Epoch 7500, loss = 0.0002\n",
      "Epoch 7550, loss = 0.0002\n",
      "Epoch 7600, loss = 0.0002\n",
      "Epoch 7650, loss = 0.0002\n",
      "Epoch 7700, loss = 0.0002\n",
      "Epoch 7750, loss = 0.0002\n",
      "Epoch 7800, loss = 0.0002\n",
      "Epoch 7850, loss = 0.0002\n",
      "Epoch 7900, loss = 0.0002\n",
      "Epoch 7950, loss = 0.0002\n",
      "Epoch 8000, loss = 0.0002\n",
      "Epoch 8050, loss = 0.0002\n",
      "Epoch 8100, loss = 0.0002\n",
      "Epoch 8150, loss = 0.0002\n",
      "Epoch 8200, loss = 0.0002\n",
      "Epoch 8250, loss = 0.0002\n",
      "Epoch 8300, loss = 0.0002\n",
      "Epoch 8350, loss = 0.0001\n",
      "Epoch 8400, loss = 0.0001\n",
      "Epoch 8450, loss = 0.0001\n",
      "Epoch 8500, loss = 0.0001\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    loss = model.step(X_train, Y_train)\n",
    "    if epoch % report_frequency == 0:\n",
    "        print(f'Epoch {epoch:4d}, loss = {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test).argmax(1)\n",
    "y_test_true = Y_test.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy = 0.976\n"
     ]
    }
   ],
   "source": [
    "print(f'test set accuracy = {accuracy_score(y_test_true, y_test_pred):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
